# global_decoder.py
import sys
import json
import re
from pathlib import Path

# Add src to path
src_dir = Path(__file__).parent / "src"
sys.path.insert(0, str(src_dir))

# Import EthiopicUtils directly
from language_utils.EthiopicUtils import EthiopicUtils


class GlobalSplinterDecoder:
    """
    Global Splinter Decoder - FINAL VERSION
    Handles both attached and space-separated formats correctly
    Includes punctuation splitting for perfect bracket handling
    """
    
    def __init__(self, language_utils):
        self.language_utils = language_utils
        self.decode_map = None
        self.cjk_range = (0x4E00, 0x9FFF)
        self.decode_cache = {}
    
    def load_decode_map(self, map_data):
        """Load decode map from dictionary."""
        self.decode_map = map_data
        print(f"‚úÖ Loaded {len(self.decode_map)} mappings")
        return True
    
    def _is_cjk_char(self, char):
        """Check if character is in CJK Unified Ideographs range."""
        if len(char) != 1:
            return False
        return 0x4E00 <= ord(char) <= 0x9FFF
    
    def _reconstruct_word(self, combined_string):
        """
        Reconstruct word using Ghost Index mapping.
        Maps CJK instructions to Ge'ez letters only, ignoring punctuation.
        """
        cache_key = combined_string
        if cache_key in self.decode_cache:
            return self.decode_cache[cache_key]
        
        # Convert to list for modification
        result = list(combined_string)
        
        # üéØ STEP 1: Find indices of actual Ge'ez letters only
        geez_indices = [
            i for i, char in enumerate(result)
            if self.language_utils.is_letter_in_language(char)
        ]
        
        # Collect all CJK chars and their instructions
        cjk_chars = [c for c in combined_string if self._is_cjk_char(c)]
        
        # üéØ STEP 2: Parse all instructions from CJK tags
        instructions = []  # (linguistic_position, marker)
        for cjk in cjk_chars:
            if cjk in self.decode_map:
                reduction_key = self.decode_map[cjk]
                parts = reduction_key.split(',')
                for part in parts:
                    part = part.strip()
                    if ':' in part:
                        try:
                            pos_str, marker = part.split(':', 1)
                            instructions.append((int(pos_str), marker.strip()))
                        except ValueError:
                            continue
        
        # Sort instructions by linguistic position
        instructions.sort(key=lambda x: x[0])
        
        # üéØ STEP 3: Apply vowels using the mapped indices
        modified_geez_positions = set()
        
        for ling_pos, marker in instructions:
            if ling_pos < len(geez_indices) and ling_pos not in modified_geez_positions:
                actual_index = geez_indices[ling_pos]
                base_char = result[actual_index]
                
                order = self.language_utils.get_vowel_order_from_marker(marker)
                if order is not None:
                    result[actual_index] = self.language_utils.apply_vowel_to_consonant(base_char, order)
                    modified_geez_positions.add(ling_pos)
        
        # üéØ STEP 4: Remove all CJK characters
        final_result = [c for c in result if not self._is_cjk_char(c)]
        reconstructed = ''.join(final_result)
        
        self.decode_cache[cache_key] = reconstructed
        return reconstructed
    
    def decode_line(self, line):
        """
        Decode a line - handles both attached and space-separated formats.
        FINAL VERSION with punctuation splitting for perfect results.
        """
        if not line or not line.strip():
            return line
        
        # üéØ FIX: Split tokens but keep punctuation separated if they are attached to words
        # This prevents ']' from being part of the '·à®·â∞·à®' skeleton.
        line = re.sub(r'([\[\](){},.;:!·ç°·ç¢·ç£·ç§·ç•·ç¶·çß·ç†·ç®])', r' \1 ', line)
        
        # Remove ‚ñÅ markers
        line = line.replace('‚ñÅ', '')
        
        # Fix number patterns
        line = re.sub(r'(\d+)\s*\(\s*:\s*', r'\1(:', line)
        
        # Split into tokens
        tokens = line.strip().split()
        
        # Result stack - builds output incrementally
        result_stack = []
        i = 0
        n = len(tokens)
        
        # Track if this line has CJK for debugging
        has_cjk = any(self._is_cjk_char(c) for token in tokens for c in token)
        if has_cjk:
            print(f"\n{'='*60}")
            print(f">>> DECODER PROCESSING: '{line}'")
            print(f">>> Tokens: {tokens}")
            print(f"{'='*60}")
        
        while i < n:
            current = tokens[i]
            
            # Check if current token contains CJK
            has_cjk_here = any(self._is_cjk_char(c) for c in current)
            
            if has_cjk_here:
                # Check if skeleton is in this token (attached case)
                contains_geez = any(self.language_utils.is_letter_in_language(c) for c in current)
                
                if contains_geez:
                    # ATTACHED CASE: e.g., '·àà·ã®‰∏â‰∏É' or '1(:·ä†·à®·â∞‰∏ù‰∏è‰∏Å'
                    reconstructed = self._reconstruct_word(current)
                    result_stack.append(reconstructed)
                    if has_cjk:
                        print(f">>> Attached token '{current}' ‚Üí '{reconstructed}'")
                    i += 1
                else:
                    # DISCONNECTED CASE: e.g., '·ä† ·à® ·â∞' then '‰∏ù‰∏è‰∏Å'
                    # Collect all consecutive CJK tokens
                    cjk_tokens = [current]
                    i += 1
                    while i < n and any(self._is_cjk_char(c) for c in tokens[i]):
                        cjk_tokens.append(tokens[i])
                        i += 1
                    cjk_string = ''.join(cjk_tokens)
                    
                    # Look backwards to find Ge'ez tokens that form the skeleton
                    skeleton_tokens = []
                    j = len(result_stack) - 1
                    while j >= 0:
                        stack_item = result_stack[j]
                        if any(self.language_utils.is_letter_in_language(c) for c in stack_item):
                            skeleton_tokens.insert(0, result_stack.pop(j))
                            j -= 1
                        else:
                            # Stop at first non-Ge'ez (punctuation)
                            break
                    
                    if skeleton_tokens:
                        # Combine skeleton tokens with CJK
                        combined = ''.join(skeleton_tokens) + cjk_string
                        reconstructed = self._reconstruct_word(combined)
                        result_stack.append(reconstructed)
                        if has_cjk:
                            print(f">>> Reconstructed '{combined}' ‚Üí '{reconstructed}'")
                    else:
                        # Fallback: keep non-CJK parts
                        non_cjk = ''.join([c for c in cjk_string if not self._is_cjk_char(c)])
                        if non_cjk:
                            result_stack.append(non_cjk)
            else:
                # Regular word or punctuation
                result_stack.append(current)
                i += 1
        
        # Join stack with spaces
        final_output = ' '.join(result_stack)
        
        # üßº Final cleanup: Remove structural markers and normalize
        final_output = final_output.replace('‚ñÅ', '')
        final_output = re.sub(r'\s+([·ç°·ç¢·ç£·ç§·ç•·ç¶·çß·ç†·ç®\(\)\[\]\{\}.,;:!?])', r'\1', final_output)
        final_output = re.sub(r'([·ç°·ç¢·ç£·ç§·ç•·ç¶·çß·ç†·ç®\(\)\[\]\{\}.,;:!?])\s+', r'\1', final_output)
        final_output = re.sub(r'\(\s*:', r'(:', final_output)
        final_output = re.sub(r':\s*\)', r':)', final_output)
        final_output = re.sub(r'\s+', ' ', final_output)
        
        return final_output.strip()


def main():
    print("="*60)
    print("GLOBAL DECODER - FINAL VERSION")
    print("="*60)
    
    # Initialize
    print("\n1. Initializing EthiopicUtils...")
    utils = EthiopicUtils()
    
    # Create decoder
    print("\n2. Creating decoder...")
    decoder = GlobalSplinterDecoder(utils)
    
    # Your actual mapping
    decode_map = {
        "‰∏Ä": "3:[√Ø]",
        "‰∏Å": "2:[√Ø]",
        "‰∏Ç": "4:[√Ø]",
        "‰∏É": "1:[√Ø]",
        "‰∏Ñ": "5:[√Ø]",
        "‰∏Ö": "0:[√Ø]",
        "‰∏Ü": "2:[a]",
        "‰∏á": "1:[a]",
        "‰∏à": "3:[a]",
        "‰∏â": "0:[a]",
        "‰∏ä": "6:[√Ø]",
        "‰∏ã": "2:[o]",
        "‰∏å": "3:[u]",
        "‰∏ç": "4:[a]",
        "‰∏é": "3:[o]",
        "‰∏è": "1:[i]",
        "‰∏ê": "4:[u]",
        "‰∏ë": "1:[o]",
        "‰∏í": "2:[u]",
        "‰∏ì": "2:[i]",
        "‰∏î": "0:[u]",
        "‰∏ï": "5:[a]",
        "‰∏ñ": "1:[u]",
        "‰∏ó": "3:[i]",
        "‰∏ò": "0:[i]",
        "‰∏ô": "0:[e]",
        "‰∏ö": "7:[√Ø]",
        "‰∏õ": "4:[o]",
        "‰∏ú": "1:[e]",
        "‰∏ù": "0:[o]",
        "‰∏û": "2:[e]",
        "‰∏ü": "5:[u]",
        "‰∏†": "4:[i]",
        "‰∏°": "3:[e]",
        "‰∏¢": "5:[i]",
        "‰∏£": "1:[9]",
        "‰∏§": "8:[√Ø]",
        "‰∏•": "2:[7]",
        "‰∏¶": "6:[a]",
        "‰∏ß": "6:[u]",
        "‰∏®": "5:[o]",
        "‰∏©": "3:[7]",
        "‰∏™": "0:[9]",
        "‰∏´": "2:[9]",
        "‰∏¨": "3:[9]",
        "‰∏≠": "4:[e]",
        "‰∏Æ": "5:[e]",
        "‰∏Ø": "1:[7]",
        "‰∏∞": "4:[7]",
        "‰∏±": "4:[9]",
        "‰∏≤": "6:[i]",
        "‰∏≥": "6:[o]",
        "‰∏¥": "7:[a]",
        "‰∏µ": "9:[√Ø]",
        "‰∏∂": "7:[u]",
        "‰∏∑": "8:[a]",
        "‰∏∏": "10:[√Ø]",
        "‰∏π": "0:[7]",
        "‰∏∫": "0:[11]",
        "‰∏ª": "8:[u]",
        "‰∏º": "7:[o]",
        "‰∏Ω": "7:[i]",
        "‰∏æ": "5:[7]",
        "‰∏ø": "6:[e]",
        "‰πÄ": "9:[a]",
        "‰πÅ": "1:[11]",
        "‰πÇ": "8:[i]",
        "‰πÉ": "8:[o]",
        "‰πÑ": "10:[a]",
        "‰πÖ": "9:[u]",
        "‰πÜ": "11:[√Ø]",
        "‰πá": "9:[i]",
        "‰πà": "9:[o]",
        "‰πâ": "10:[u]",
        "‰πä": "11:[a]",
        "‰πã": "5:[9]",
        "‰πå": "8:[e]",
        "‰πç": "7:[e]",
        "‰πé": "2:[11]",
        "‰πè": "10:[i]",
        "‰πê": "12:[√Ø]",
        "‰πë": "6:[7]",
        "‰πí": "10:[o]",
        "‰πì": "12:[a]",
        "‰πî": "9:[e]",
        "‰πï": "11:[u]",
        "‰πñ": "7:[7]",
        "‰πó": "3:[11]",
        "‰πò": "11:[i]",
        "‰πô": "13:[√Ø]",
        "‰πö": "14:[√Ø]",
        "‰πõ": "6:[9]",
        "‰πú": "8:[7]",
        "‰πù": "10:[e]",
        "‰πû": "11:[o]",
        "‰πü": "12:[u]",
        "‰π†": "4:[11]",
        "‰π°": "9:[7]",
        "‰π¢": "2:[8]",
        "‰π£": "13:[a]",
        "‰π§": "15:[√Ø]",
        "‰π•": "16:[√Ø]",
        "‰π¶": "0:[8]",
        "‰πß": "11:[e]",
        "‰π®": "12:[o]",
        "‰π©": "0:[√Ø],1:[√Ø],2:[√Ø]",
        "‰π™": "0:[a],1:[√Ø]",
        "‰π´": "0:[√Ø],1:[√Ø]",
        "‰π¨": "1:[√Ø],2:[√Ø]",
        "‰π≠": "0:[√Ø],1:[a]",
        "‰πÆ": "1:[a],2:[√Ø]",
        "‰πØ": "1:[i],2:[√Ø]",
        "‰π∞": "1:[√Ø],2:[o]",
        "‰π±": "0:[a],2:[√Ø]",
        "‰π≤": "0:[√Ø],1:[√Ø],2:[√Ø],3:[√Ø]",
        "‰π≥": "1:[√Ø],3:[√Ø]",
        "‰π¥": "0:[√Ø],1:[a],2:[√Ø]",
        "‰πµ": "2:[a],3:[√Ø]",
        "‰π∂": "1:[√Ø],2:[√Ø],3:[√Ø]",
        "‰π∑": "1:[√Ø],2:[a],3:[√Ø]",
    }
    
    decoder.load_decode_map(decode_map)
    
    # Test cases
    test_cases = [
        # Attached format (no spaces)
        ("1(:·ä†·à®·â∞‰∏ù‰∏è‰∏Å", "1(:·ä¶·à™·âµ"),
        ("·ãò·ã∞·åà·àò‰∏á‰∏Å‰∏Ä", "·ãò·ã≥·åç·àù"),
        ("·ç¢)·ãò·äê·â∞‰∏Ö‰∏É‰∏í", "·ç¢)·ãù·äï·â±"),
        ("·ãà·ä†·â∞‰∏Ö‰∏É‰∏í", "·ãç·ä•·â±"),
        ("·äê·åà·à®‰∏Å", "·äê·åà·à≠"),
        ("·ãò·äê·åà·à®·àò‰∏é‰∏ê", "·ãò·äê·åà·àÆ·àô"),
        ("·àò·à∞‰∏î‰∏ú", "·àô·à¥"),
        ("[ ·ä† ‰∏ô ]·à®·â∞·à®‰∏Ö‰∏É‰∏Ü", "[·ä§]·à≠·âµ·à´"),
        
        # Space-separated format
        ("1 ( : ·ä† ·à® ·â∞ ‰∏ù ‰∏è‰∏Å", "1(:·ä¶·à™·âµ"),
        ("·ãò ·ã∞ ·åà ·àò ‰∏á ‰∏Å ‰∏Ä", "·ãò·ã≥·åç·àù"),
        ("·ç¢ ) ·ãò ·äê ·â∞ ‰∏Ö ‰∏É ‰∏í", "·ç¢)·ãù·äï·â±"),
        ("·ãà ·ä† ·â∞ ‰∏Ö ‰∏É ‰∏í", "·ãç·ä•·â±"),
        ("·äê ·åà ·à® ‰∏Å", "·äê·åà·à≠"),
        ("[ ·ä† ‰∏ô ] ·à®·â∞ ·à® ‰∏Ö‰∏É‰∏Ü", "[·ä§]·à≠·âµ·à´"),
        
        # Problematic attached cases
        ("·àà·ã®‰∏â‰∏É", "·àã·ã≠"),
        ("·ä†·à∞·ä®·à®·äê‰∏É‰∏°‰∏Ç", "·ä†·àµ·ä®·à¨·äï"),
        ("·â†·àà·çà·ãà‰∏â‰∏Ä", "·â£·àà·çà·ãç"),
        ("·âÄ·ã®·â∞·ãà·àà‰∏ù‰∏É‰∏à‰∏Ç·ç°·ç°", "·âÜ·ã≠·â∞·ãã·àç·ç°·ç°"),
        ("·ä®·à∞·àò·äê·â∞‰∏á‰∏Å‰∏Ä‰∏Ç", "·ä®·à≥·àù·äï·âµ"),
        ("·ã®·ä†·àà·â∞‰∏É‰∏å", "·ã®·ä•·àà·â±"),
    ]
    
    print("\n" + "="*60)
    print("TEST RESULTS")
    print("="*60)
    
    passed = 0
    failed = 0
    
    for i, (input_text, expected) in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"RUNNING TEST {i}")
        print(f"{'='*60}")
        result = decoder.decode_line(input_text)
        status = "‚úÖ PASSED" if result == expected else "‚ùå FAILED"
        if result == expected:
            passed += 1
        else:
            failed += 1
        
        print(f"\n{i}. Input:  '{input_text}'")
        print(f"   Result: '{result}'")
        print(f"   Expect: '{expected}'")
        print(f"   {status}")
        
        if result != expected:
            # Show what went wrong
            if result.replace(' ', '') == expected.replace(' ', ''):
                print(f"      Issue: Extra/missing spaces")
            else:
                # Show character by character
                for j, (r_char, e_char) in enumerate(zip(result, expected)):
                    if r_char != e_char:
                        print(f"      First diff at position {j}: '{r_char}' vs '{e_char}'")
                        break
    
    print(f"\n{'='*60}")
    print(f"SUMMARY: {passed} passed, {failed} failed")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()